{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import gym\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discount factor for future utilities\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "\n",
    "#number of episodes to run\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "#max steps per episode\n",
    "MAX_STEPS = 10000\n",
    "\n",
    "#score agent needs for environment to be solved\n",
    "SOLVED_SCORE = 195\n",
    "\n",
    "#device to run model on \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a neural network to learn our policy parameters\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \n",
    "    #Takes in observations and outputs actions\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.input_layer = nn.Linear(observation_space, 128)\n",
    "        self.output_layer = nn.Linear(128, action_space)\n",
    "    \n",
    "    #forward pass\n",
    "    def forward(self, x):\n",
    "        #input states\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        #relu activation\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        #actions\n",
    "        actions = self.output_layer(x)\n",
    "        \n",
    "        #get softmax for a probability distribution\n",
    "        action_probs = F.softmax(actions, dim=1)\n",
    "        \n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a neural network to learn state value\n",
    "class StateValueNetwork(nn.Module):\n",
    "    \n",
    "    #Takes in state\n",
    "    def __init__(self, observation_space):\n",
    "        super(StateValueNetwork, self).__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(observation_space, 128)\n",
    "        self.output_layer = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #input layer\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        #activiation relu\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        #get state value\n",
    "        state_value = self.output_layer(x)\n",
    "        \n",
    "        return state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(network, state):\n",
    "    ''' Selects an action given current state\n",
    "    Args:\n",
    "    - network (Torch NN): network to process state\n",
    "    - state (Array): Array of action space in an environment\n",
    "    \n",
    "    Return:\n",
    "    - (int): action that is selected\n",
    "    - (float): log probability of selecting that action given state and network\n",
    "    '''\n",
    "    \n",
    "    #convert state to float tensor, add 1 dimension, allocate tensor on device\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    #use network to predict action probabilities\n",
    "    action_probs = network(state)\n",
    "    state = state.detach()\n",
    "    \n",
    "    #sample an action using the probability distribution\n",
    "    m = Categorical(action_probs)\n",
    "    action = m.sample()\n",
    "    \n",
    "    #return action\n",
    "    return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "#Init network\n",
    "policy_network = PolicyNetwork(env.observation_space.shape[0], env.action_space.n).to(DEVICE)\n",
    "stateval_network = StateValueNetwork(env.observation_space.shape[0]).to(DEVICE)\n",
    "\n",
    "#Init optimizer\n",
    "policy_optimizer = optim.Adam(policy_network.parameters(), lr=0.001)\n",
    "stateval_optimizer = optim.Adam(stateval_network.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\calvi\\AppData\\Local\\Temp\\ipykernel_23992\\2987005609.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for episode in tqdm_notebook(range(NUM_EPISODES)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97361b4a196848d9b5dbab4c1538e1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#track scores\n",
    "scores = []\n",
    "\n",
    "#track recent scores\n",
    "recent_scores = deque(maxlen = 100)\n",
    "\n",
    "#run episodes\n",
    "for episode in tqdm_notebook(range(NUM_EPISODES)):\n",
    "    \n",
    "    #init variables\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    I = 1\n",
    "    \n",
    "    #run episode, update online\n",
    "    for step in range(MAX_STEPS):\n",
    "        \n",
    "        #get action and log probability\n",
    "        action, lp = select_action(policy_network, state)\n",
    "        \n",
    "        #step with action\n",
    "        new_state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        #update episode score\n",
    "        score += reward\n",
    "        \n",
    "        #get state value of current state\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
    "        state_val = stateval_network(state_tensor)\n",
    "        \n",
    "        #get state value of next state\n",
    "        new_state_tensor = torch.from_numpy(new_state).float().unsqueeze(0).to(DEVICE)        \n",
    "        new_state_val = stateval_network(new_state_tensor)\n",
    "        \n",
    "        #if terminal state, next state val is 0\n",
    "        if done:\n",
    "            new_state_val = torch.tensor([0]).float().unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        #calculate value function loss with MSE\n",
    "        val_loss = F.mse_loss(reward + DISCOUNT_FACTOR * new_state_val, state_val)\n",
    "        val_loss *= I\n",
    "        \n",
    "        #calculate policy loss\n",
    "        advantage = reward + DISCOUNT_FACTOR * new_state_val.item() - state_val.item()\n",
    "        policy_loss = -lp * advantage\n",
    "        policy_loss *= I\n",
    "        \n",
    "        #Backpropagate policy\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward(retain_graph=True)\n",
    "        policy_optimizer.step()\n",
    "        \n",
    "        #Backpropagate value\n",
    "        stateval_optimizer.zero_grad()\n",
    "        val_loss.backward()\n",
    "        stateval_optimizer.step()\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        #move into new state, discount I\n",
    "        state = new_state\n",
    "        I *= DISCOUNT_FACTOR\n",
    "    \n",
    "    #append episode score \n",
    "    scores.append(score)\n",
    "    recent_scores.append(score)\n",
    "    \n",
    "    #early stopping if we meet solved score goal\n",
    "    if np.array(recent_scores).mean() >= SOLVED_SCORE:\n",
    "        break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set()\n",
    "\n",
    "plt.plot(scores)\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('episodes')\n",
    "plt.title('Training score of CartPole Actor-Critic TD(0)')\n",
    "\n",
    "reg = LinearRegression().fit(np.arange(len(scores)).reshape(-1, 1), np.array(scores).reshape(-1, 1))\n",
    "y_pred = reg.predict(np.arange(len(scores)).reshape(-1, 1))\n",
    "plt.plot(y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#track scores\n",
    "scores = []\n",
    "\n",
    "#track recent scores\n",
    "recent_scores = deque(maxlen = 100)\n",
    "\n",
    "#run episodes\n",
    "for episode in tqdm_notebook(range(10)):\n",
    "    \n",
    "    #init variables\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    I = 1\n",
    "    \n",
    "    #run episode, update online\n",
    "    for step in range(MAX_STEPS):\n",
    "        \n",
    "        #get action and log probability\n",
    "        action, lp = select_action(policy_network, state)\n",
    "        \n",
    "        #step with action\n",
    "        new_state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        #update episode score\n",
    "        score += reward\n",
    "        \n",
    "        #get state value of current state\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
    "        state_val = stateval_network(state_tensor)\n",
    "        \n",
    "        #get state value of next state\n",
    "        new_state_tensor = torch.from_numpy(new_state).float().unsqueeze(0).to(DEVICE)        \n",
    "        new_state_val = stateval_network(new_state_tensor)\n",
    "        \n",
    "        #if terminal state, next state val is 0\n",
    "        if done:\n",
    "            new_state_val = torch.tensor([0]).float().unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        #calculate value function loss with MSE\n",
    "        val_loss = F.mse_loss(reward + DISCOUNT_FACTOR * new_state_val, state_val)\n",
    "        val_loss *= I\n",
    "        \n",
    "        #calculate policy loss\n",
    "        advantage = reward + DISCOUNT_FACTOR * new_state_val.item() - state_val.item()\n",
    "        policy_loss = -lp * advantage\n",
    "        policy_loss *= I\n",
    "        \n",
    "        #Backpropagate policy\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward(retain_graph=True)\n",
    "        policy_optimizer.step()\n",
    "        \n",
    "        #Backpropagate value\n",
    "        stateval_optimizer.zero_grad()\n",
    "        val_loss.backward()\n",
    "        stateval_optimizer.step()\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        #move into new state, discount I\n",
    "        state = new_state\n",
    "        I *= DISCOUNT_FACTOR\n",
    "    \n",
    "    #append episode score \n",
    "    scores.append(score)\n",
    "    recent_scores.append(score)\n",
    "    \n",
    "    #early stopping if we meet solved score goal\n",
    "    if np.array(recent_scores).mean() >= SOLVED_SCORE:\n",
    "        break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set()\n",
    "\n",
    "plt.plot(scores)\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('episodes')\n",
    "plt.title('Training score of CartPole Actor-Critic TD(0)')\n",
    "\n",
    "reg = LinearRegression().fit(np.arange(len(scores)).reshape(-1, 1), np.array(scores).reshape(-1, 1))\n",
    "y_pred = reg.predict(np.arange(len(scores)).reshape(-1, 1))\n",
    "plt.plot(y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "state = env.reset()\n",
    "scores = []\n",
    "\n",
    "for _ in tqdm_notebook(range(50)):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action, lp = select_action(policy_network, state)\n",
    "        new_state, reward, done, info, _ = env.step(action)\n",
    "        score += reward\n",
    "        state = new_state\n",
    "    scores.append(score)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
