{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#add environment to PYTHONPATH\n",
    "import sys\n",
    "import os\n",
    "env_path = os.path.join(os.path.abspath(os.getcwd()), '..\\\\Environments\\\\ContinuousCartPole')\n",
    "sys.path.append(env_path)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from continuous_cartpole import ContinuousCartPoleEnv\n",
    "\n",
    "import gym\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "from copy import deepcopy, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicPolicy(nn.Module):\n",
    "    \n",
    "    def __init__(self, obs_space, action_space):\n",
    "        super(DeterministicPolicy, self).__init__()\n",
    "        self.input_batch_norm = nn.BatchNorm1d(obs_space)\n",
    "        \n",
    "        self.first_layer = nn.Linear(obs_space, 400)\n",
    "        self.first_batch_norm = nn.BatchNorm1d(400)\n",
    "        \n",
    "        self.second_layer = nn.Linear(400, 300)\n",
    "        self.second_batch_norm = nn.BatchNorm1d(300)\n",
    "        \n",
    "        self.output_layer = nn.Linear(300, action_space)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_batch_norm(x)\n",
    "        \n",
    "        x = self.first_layer(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.first_batch_norm(x)\n",
    "        \n",
    "        x = self.second_layer(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.second_batch_norm(x)\n",
    "        \n",
    "        output = self.output_layer(x)\n",
    "        actions = torch.tanh(output)\n",
    "        \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, obs_space, action_space):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.input_batch_norm = nn.BatchNorm1d(obs_space)\n",
    "        \n",
    "        self.first_layer = nn.Linear(obs_space, 400)\n",
    "        self.first_batch_norm = nn.BatchNorm1d(400)\n",
    "        \n",
    "        self.second_layer = nn.Linear(400 + action_space, 300)\n",
    "        \n",
    "        self.output_layer = nn.Linear(300, 1)\n",
    "        \n",
    "    def forward(self, x, actions):\n",
    "        x = self.input_batch_norm(x)\n",
    "        \n",
    "        x = self.first_layer(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.first_batch_norm(x)\n",
    "        \n",
    "        x_with_action = torch.cat([x, actions], dim=1)\n",
    "        \n",
    "        x = self.second_layer(x_with_action)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        q_val = self.output_layer(x)\n",
    "        \n",
    "        return q_val\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size=1, mu=0, theta=1.0, sigma=0.1):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(policy, state, is_random):\n",
    "    \n",
    "    state_tensor = torch.from_numpy(state).float().unsqueeze(0) \n",
    "    action = policy(state_tensor)\n",
    "    action = action + ou.sample().item()\n",
    "    action = action.squeeze(0).detach().numpy()\n",
    "    \n",
    "    if is_random:\n",
    "        action = env.action_space.sample()\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(batch, policy_network, q_network, target_policy_network, target_q_network, policy_optimizer, q_optimizer, τ):\n",
    "    \n",
    "    action_batch = torch.Tensor([item[1] for item in batch]).unsqueeze(1)\n",
    "    state_batch = torch.Tensor([item[0] for item in batch])\n",
    "    new_state_batch = torch.Tensor([item[3] for item in batch])\n",
    "    rewards_batch = [item[2] for item in batch]\n",
    "    \n",
    "    train([policy_network, q_network, target_policy_network, target_q_network])\n",
    "    \n",
    "    #train critic\n",
    "    next_action_batch = target_policy_network(new_state_batch)\n",
    "    next_q_batch = target_q_network(new_state_batch, next_action_batch)\n",
    "    q_batch = q_network(state_batch, action_batch)\n",
    "    \n",
    "    target_q_batch = reward + γ * next_q_batch\n",
    "    critic_loss = F.mse_loss(target_q_batch, q_batch)  \n",
    "    \n",
    "    q_optimizer.zero_grad()\n",
    "    critic_loss.backward(retain_graph = True)\n",
    "    q_optimizer.step()\n",
    "    \n",
    "    #train actor\n",
    "    actor_loss = -torch.mean(q_network(state_batch, policy_network(state_batch)))\n",
    "\n",
    "    policy_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "    \n",
    "    update_target_net(policy_network, target_policy_network, τ)\n",
    "    update_target_net(q_network, target_q_network, τ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(models):\n",
    "    for model in models:\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(models):\n",
    "    for model in models:\n",
    "        model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_net(net, target_net, τ):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param, target_param in zip(net.parameters(), target_net.parameters()):\n",
    "            new_param = τ * param.data + (1 - τ) * target_param.data\n",
    "            target_param.data.copy_(new_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discount factor\n",
    "γ = 0.99\n",
    "\n",
    "#soft target constant\n",
    "τ = 0.001\n",
    "\n",
    "#Learning rates\n",
    "α_θ = 0.0001\n",
    "αw = 0.001\n",
    "\n",
    "#episode to run\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "#steps per episode\n",
    "MAX_STEPS = 5000\n",
    "\n",
    "#batch size\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "#replay buffer\n",
    "BUFFER_SIZE = 100000\n",
    "\n",
    "#minimum buffer size to train\n",
    "MIN_REPLAY = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init environment\n",
    "env_id = 'MountainCarContinuous-v0'\n",
    "env = gym.make(env_id).env\n",
    "env = ContinuousCartPoleEnv()\n",
    "\n",
    "#env parameters\n",
    "obs_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.shape[0]\n",
    "\n",
    "#set seeds\n",
    "np.random.seed(777)\n",
    "random.seed(777)\n",
    "env.seed(777)\n",
    "torch.manual_seed(777)\n",
    "\n",
    "#init networks\n",
    "policy_network = DeterministicPolicy2(obs_space, action_space)\n",
    "q_network = QNetwork2(obs_space, action_space)\n",
    "\n",
    "\n",
    "target_policy_network = DeterministicPolicy2(obs_space, action_space)\n",
    "target_q_network = QNetwork2(obs_space, action_space)\n",
    "\n",
    "\n",
    "#target network same weights\n",
    "for param, target_param in zip(policy_network.parameters(), target_policy_network.parameters()):\n",
    "    param.data.copy_(target_param.data)\n",
    "    \n",
    "for param, target_param in zip(q_network.parameters(), target_q_network.parameters()):\n",
    "    param.data.copy_(target_param.data)\n",
    "\n",
    "\n",
    "#init optimizers\n",
    "policy_optimizer = optim.Adam(policy_network.parameters(), lr=α_θ)\n",
    "q_optimizer = optim.Adam(q_network.parameters(), lr=αw, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b22d424c4ec41e98633effa48572eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "total_steps = 0\n",
    "\n",
    "for episode in tqdm_notebook(range(NUM_EPISODES*3)):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    done = False\n",
    "    ou = OUNoise()\n",
    "    \n",
    "    for step in range(MAX_STEPS):\n",
    "        #env.render()\n",
    "        total_steps += 1\n",
    "        \n",
    "        eval([policy_network])\n",
    "        action = select_action(policy_network, state, total_steps > MIN_REPLAY)\n",
    "        \n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        score += reward\n",
    "        \n",
    "        #reward += 100*((np.sin(3*new_state[0]) * 0.0025 + 0.5 * new_state[1] * new_state[1]) - (np.sin(3*state[0]) * 0.0025 + 0.5 * state[1] * state[1]))\n",
    "        \n",
    "        replay_buffer.append([state, action.item(), reward, new_state])\n",
    "        \n",
    "        if len(replay_buffer) >= BATCH_SIZE and total_steps > MIN_REPLAY:\n",
    "            batch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "            update_parameters(batch, policy_network, q_network, target_policy_network, target_q_network, policy_optimizer, q_optimizer, τ)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "    scores.append(score)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-224-baa45cf6ac7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'grey'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Training score'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'blue'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Target Policy score'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'score'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'episodes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'policy_scores' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set()\n",
    "\n",
    "plt.plot(scores, color='grey', label='Training score')\n",
    "plt.plot(policy_scores, color='blue', label='Target Policy score')\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('episodes')\n",
    "plt.title('Score history of MountainCarContinuous with DDPG')\n",
    "plt.legend()\n",
    "\n",
    "reg = LinearRegression().fit(np.arange(len(policy_scores)).reshape(-1, 1), np.array(policy_scores).reshape(-1, 1))\n",
    "y_pred = reg.predict(np.arange(len(policy_scores)).reshape(-1, 1))\n",
    "plt.plot(y_pred, color='orange')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    for step in range(MAX_STEPS):\n",
    "        env.render()\n",
    "\n",
    "        new_state, reward, done, info = env.step(env.action_space.sample())\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58e4ea1a9c44307bebfe97d893aa8c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "testing_scores = []\n",
    "\n",
    "for _ in tqdm_notebook(range(5)):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    for step in range(MAX_STEPS):\n",
    "        #env.render()\n",
    "        eval([policy_network])\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        action = policy_network(state_tensor)\n",
    "        new_state, reward, done, info = env.step([action.item()])\n",
    "        \n",
    "        score += reward\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    testing_scores.append(score)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.0, 6.0, 6.0, 6.0, 5.0]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.8"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(testing_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15999999999999998"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(testing_scores).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
