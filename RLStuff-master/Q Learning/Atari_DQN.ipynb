{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Lambda\n",
    "from tqdm import tqdm_notebook\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from collections import deque\n",
    "import random\n",
    "import cv2\n",
    "import stable_baselines3.common.atari_wrappers as atari_wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import atari roms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!python -m atari_py.import_roms ROMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Initialize environment. v4 means no action repeat\n",
    "env = gym.make('PongNoFrameskip-v4')\n",
    "\n",
    "#wraps env with these preprocessing options:\n",
    "#values will be scaled at training time to save memory\n",
    "\"\"\"Atari 2600 preprocessings. \n",
    "    This class follows the guidelines in \n",
    "    Machado et al. (2018), \"Revisiting the Arcade Learning Environment: \n",
    "    Evaluation Protocols and Open Problems for General Agents\".\n",
    "    Specifically:\n",
    "    * NoopReset: obtain initial state by taking random number of no-ops on reset. \n",
    "    * Frame skipping: 4 by default\n",
    "    * Max-pooling: most recent two observations\n",
    "    * Termination signal when a life is lost: turned off by default. Not recommended by Machado et al. (2018).\n",
    "    * Resize to a square image: 84x84 by default\n",
    "    * Grayscale observation: optional\n",
    "    * Scale observation: optional\"\"\"\n",
    "env = gym.wrappers.AtariPreprocessing(env, noop_max=30, frame_skip=4, \n",
    "                                      screen_size=84, terminal_on_life_loss=False, \n",
    "                                      grayscale_obs=True, grayscale_newaxis=False, scale_obs=False)\n",
    "\n",
    "env = gym.wrappers.FrameStack(env, 4)\n",
    "\n",
    "env = atari_wrappers.ClipRewardEnv(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actions in this environment\n",
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of frames to run\n",
    "NUM_FRAMES = 1000000\n",
    "\n",
    "#number of episodes to run\n",
    "NUM_EPISODES = 50\n",
    "\n",
    "#max iterations per run\n",
    "MAX_ITERATIONS = 1000000\n",
    "\n",
    "#epsilon for choosing action\n",
    "eps = 1\n",
    "\n",
    "#minimum eps\n",
    "eps_min = 0.1\n",
    "\n",
    "#eps linear decay for first 10% of run\n",
    "eps_linear_decay = (eps-eps_min)/(NUM_FRAMES/5)\n",
    "\n",
    "#discount factor for future utility\n",
    "discount_factor = 0.99\n",
    "\n",
    "#batch size for exp replay\n",
    "batch_size = 32\n",
    "\n",
    "#max memory stored for exp replay\n",
    "MAX_MEMORY = int(NUM_FRAMES/10)\n",
    "\n",
    "#initial population of memory using random policy\n",
    "INIT_MEMORY = int(NUM_FRAMES/20)\n",
    "\n",
    "#update interval to use target network\n",
    "TARGET_C = int(NUM_FRAMES/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep scores\n",
    "scores = []\n",
    "frames = 0\n",
    "\n",
    "#iterate through 10 playthroughs\n",
    "for _ in tqdm_notebook(range(1)):\n",
    "    \n",
    "    #reset env\n",
    "    env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    #while game is not over\n",
    "    while not done:\n",
    "        #render env\n",
    "        env.render()\n",
    "        frames += 1\n",
    "        \n",
    "        #execute random action\n",
    "        _, reward, done, _ = env.step(env.action_space.sample())\n",
    "        \n",
    "        #track score\n",
    "        score += reward\n",
    "        \n",
    "    #append to score list\n",
    "    scores.append(score)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning with image input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(eps, model, env, state):\n",
    "    ''' Returns an action using epsilon greedy strategy\n",
    "    Args:\n",
    "    - eps (int): chance for random action\n",
    "    - model (Model): Keras model used to choose best action\n",
    "    - env (EnvSpec): Gym environment\n",
    "    \n",
    "    Returns:\n",
    "    - (int): index of best action\n",
    "    '''\n",
    "    #exploration\n",
    "    if np.random.random() < eps:\n",
    "        #exploration\n",
    "        action = np.random.randint(0, env.action_space.n)\n",
    "        return action\n",
    "    else:\n",
    "        #exploitation\n",
    "        #use expand_dims here to add a dimension for input layer\n",
    "        q_vals = model.predict(state)\n",
    "        action = np.argmax(q_vals)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experience_replay(memory, model, target_model, discount_factor, batch_size):\n",
    "    ''' Fits the model with minibatch of states from memory\n",
    "    Args:\n",
    "    - memory (Array): array of environment transitions\n",
    "    - model (Model): Keras model to be fit\n",
    "    - target_model (Model): Keras model to get target Q val\n",
    "    - discount_factor (float): discount factor for future utility\n",
    "    - batch_size (int): size of minibatch\n",
    "    \n",
    "    Returns: None\n",
    "    '''\n",
    "    \n",
    "    #if memory is less than batch size, return nothing\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    else:\n",
    "        states = []\n",
    "        targets = []\n",
    "        \n",
    "        #sample a batch\n",
    "        minibatch = random.sample(memory, batch_size)\n",
    "        \n",
    "        #iterate through bastch\n",
    "        for state, action, reward, new_state, done in minibatch:\n",
    "            #scale states to be [0,1]. We only scale before fitting cuz storing uint8 is cheaper\n",
    "            state = state/255\n",
    "            new_state = new_state/255\n",
    "\n",
    "            target = reward\n",
    "            \n",
    "            #if game not over, target q val includes discounted future utility\n",
    "            #we use a cloned model to predict here for stability. Model is changed every C frames\n",
    "            #we use the online model to choose best action to deal with overestimation error (Double-Q learning)\n",
    "            if not done:\n",
    "                best_future_action = np.argmax(model.predict(new_state))\n",
    "                target = reward + discount_factor * target_model.predict(new_state)[0][best_future_action]\n",
    "            \n",
    "            #get current actions vector\n",
    "            target_vector = model.predict(state)[0]\n",
    "            \n",
    "            #update current action q val with target q val\n",
    "            target_vector[action] = target\n",
    "            \n",
    "            #add to states\n",
    "            states.append(state)\n",
    "            \n",
    "            #add to targets\n",
    "            targets.append(target_vector)\n",
    "            \n",
    "        #fit model\n",
    "        model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ncwh_to_nwhc(tensor):\n",
    "    '''Converts tensor from NCWH to NWHC\n",
    "    Args:\n",
    "    - tensor (4D Array): NCWH tensor\n",
    "    \n",
    "    Returns:\n",
    "    - (4D Array): tensor in NWHC format\n",
    "    '''\n",
    "    return tf.transpose(tensor, [0, 2, 3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#I use lambda layer so I can convert NCWH to NWHC since CPU training doesn't support NCWH\n",
    "model = Sequential(\n",
    "    [\n",
    "        Lambda(ncwh_to_nwhc, output_shape=(84, 84, 4), input_shape=(4, 84, 84)),\n",
    "        Conv2D(16, kernel_size=(8, 8), strides=4, activation=\"relu\", input_shape=(4, 84, 84)),\n",
    "        Conv2D(32, kernel_size=(4, 4), strides=2, activation=\"relu\"),\n",
    "        Flatten(),\n",
    "        Dense(256, activation=\"relu\"),\n",
    "        Dense(env.action_space.n, activation=\"linear\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "rms = tf.keras.optimizers.RMSprop(learning_rate=0.00025, momentum=0.95, epsilon=0.01)\n",
    "model.compile(loss=tf.keras.losses.Huber(), optimizer=rms)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = Sequential(\n",
    "    [\n",
    "        Lambda(ncwh_to_nwhc, output_shape=(84, 84, 4), input_shape=(4, 84, 84)),\n",
    "        Conv2D(16, kernel_size=(8, 8), strides=4, activation=\"relu\", input_shape=(4, 84, 84)),\n",
    "        Conv2D(32, kernel_size=(4, 4), strides=2, activation=\"relu\"),\n",
    "        Flatten(),\n",
    "        Dense(256, activation=\"relu\"),\n",
    "        Dense(env.action_space.n, activation=\"linear\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prefill memory with INIT_MEMORY frames\n",
    "\n",
    "#init memory using deque to only store MAX_MEMORY\n",
    "memory = deque(maxlen=MAX_MEMORY)\n",
    "\n",
    "#progress bar\n",
    "pbar = tqdm_notebook(total=INIT_MEMORY)\n",
    "\n",
    "#playthrough game until memory is prefilled\n",
    "while len(memory) < INIT_MEMORY:\n",
    "    \n",
    "    #reset env\n",
    "    state = env.reset()\n",
    "\n",
    "    done = False\n",
    "    \n",
    "    #playthrough\n",
    "    while not done:\n",
    "        \n",
    "        #random action\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        #execute action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        #add transition to memory\n",
    "        memory.append([np.expand_dims(state, axis=0), action, reward, np.expand_dims(new_state, axis=0), done])\n",
    "        \n",
    "        #progress bar\n",
    "        pbar.update(1)\n",
    "        \n",
    "        #update state\n",
    "        state = new_state\n",
    "        \n",
    "#close progress bar\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init scores\n",
    "scores = []\n",
    "\n",
    "#init total_frames\n",
    "total_frames = 0\n",
    "\n",
    "#init num_updates\n",
    "num_updates = 0\n",
    "\n",
    "#init fitness history\n",
    "fit_hist = {'loss': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pbar = tqdm_notebook(total=50)\n",
    "\n",
    "#run frames\n",
    "while total_frames < NUM_FRAMES:\n",
    "        \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    frames = 0\n",
    "            \n",
    "    #playing through this round\n",
    "    for frame in range(MAX_ITERATIONS):\n",
    "        env.render()\n",
    "        \n",
    "        frames += 1\n",
    "        \n",
    "        #epsilon greedy choose action\n",
    "        action = epsilon_greedy(eps, model, env, np.expand_dims(state, axis=0))\n",
    "        \n",
    "        \n",
    "        #execute action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        #track score\n",
    "        score += reward\n",
    "        \n",
    "        #memorize\n",
    "        memory.append([np.expand_dims(state, axis=0), action, reward, np.expand_dims(new_state, axis=0), done])\n",
    "        \n",
    "        #exp replay\n",
    "        experience_replay(memory, model, model, discount_factor, batch_size)\n",
    "        \n",
    "        #clone target network every C frames\n",
    "        num_updates += batch_size\n",
    "        \n",
    "        if num_updates > TARGET_C:\n",
    "            num_updates = 0\n",
    "            target_model.set_weights(model.get_weights())\n",
    "            \n",
    "            #save memory and model\n",
    "            np.save('memory', memory)\n",
    "            model.save('tmp_model')\n",
    "            \n",
    "        \n",
    "        #update state\n",
    "        state = new_state\n",
    "        \n",
    "        #decay epsilon\n",
    "        eps -= eps_linear_decay\n",
    "        eps = max(eps, eps_min)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    scores.append(score)\n",
    "    total_frames += frames\n",
    "    pbar.update(1)\n",
    "    \n",
    "pbar.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "plt.plot(scores)\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('episodes')\n",
    "plt.title('CartPole')\n",
    "\n",
    "reg = LinearRegression().fit(np.arange(len(scores)).reshape(-1, 1), np.array(scores).reshape(-1, 1))\n",
    "y_pred = reg.predict(np.arange(len(scores)).reshape(-1, 1))\n",
    "plt.plot(y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying policy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "done = False\n",
    "score = 0\n",
    "state = env.reset()\n",
    "q_hist = []\n",
    "scores = []\n",
    "\n",
    "for _ in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        state = np.array(state)/255\n",
    "        action = np.argmax(model.predict(np.expand_dims(state, axis=0)))\n",
    "        q_hist.append(model.predict(np.expand_dims(state, axis=0)).mean())\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        state = new_state\n",
    "    scores.append(score)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('tmp_model', custom_objects={'tf':tf})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-68.83295 , -40.79094 , -52.03697 , -41.054867, -47.033543,\n",
       "        -73.834114]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.expand_dims(env.reset(), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "728c223486afcc150a80a1652d40dbe61283ae708253ba05ab4e921fdaff8aab"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
